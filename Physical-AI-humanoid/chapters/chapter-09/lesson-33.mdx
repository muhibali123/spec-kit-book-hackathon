---
title: "Model Predictive Control for Humanoids"
description: "Learn how humanoid robots use predictive models to optimize their control actions for complex dynamic behaviors."
tags: ["model predictive control", "MPC", "control systems", "humanoid robotics", "optimization"]
---

import { Tabs, TabItem } from '@docusaurus/theme-common';

# Model Predictive Control for Humanoids

## Learning Objectives

By the end of this lesson, students will be able to:
- Explain the principles of Model Predictive Control (MPC) in humanoid robotics
- Implement basic MPC algorithms for humanoid control
- Understand the advantages of MPC over traditional control methods
- Analyze the computational challenges of MPC for real-time humanoid control
- Evaluate the role of prediction horizons in control performance

## Key Concepts

- **Model Predictive Control (MPC)**: Control strategy that uses a model to predict future system behavior
- **Prediction Horizon**: Time window over which future states are predicted
- **Control Horizon**: Time window over which control actions are optimized
- **Cost Function**: Mathematical function that quantifies the performance of control actions
- **State Constraints**: Limits on the system's state variables during control
- **Control Constraints**: Limits on the control inputs that can be applied
- **Rolling Horizon**: Receding horizon approach where control is updated at each time step
- **Linear Quadratic Regulator (LQR)**: Special case of MPC for linear systems with quadratic costs

## Theory Summary

Model Predictive Control (MPC) is an advanced control strategy that has become increasingly important for humanoid robotics due to its ability to handle complex dynamics, constraints, and multi-objective optimization. Unlike traditional control methods that compute a single control action, MPC solves an optimization problem at each time step to determine the best sequence of future control actions.

The core principle of MPC involves three main steps: prediction, optimization, and implementation. First, the controller uses a model of the system to predict how the system will behave over a future time horizon given different potential control sequences. Next, it solves an optimization problem to find the control sequence that minimizes a cost function while satisfying system constraints. Finally, only the first control action in the sequence is applied to the system, and the process repeats at the next time step.

For humanoid robots, MPC offers several key advantages. It can explicitly handle state and control constraints, which is crucial for maintaining balance and avoiding joint limits. It can also handle multiple, potentially conflicting objectives by incorporating them into the cost function. Additionally, MPC can anticipate future events and adjust control actions accordingly, making it well-suited for dynamic tasks like walking or manipulation.

The prediction horizon in MPC determines how far into the future the controller looks when making decisions. A longer horizon allows for more far-sighted control but increases computational complexity. A shorter horizon is computationally efficient but may lead to suboptimal control decisions. The optimal horizon length depends on the specific application and system dynamics.

The control horizon determines how many control actions are optimized simultaneously. In some implementations, the control horizon is shorter than the prediction horizon, meaning that control actions are held constant after a certain point. This approach can reduce computational complexity while still providing good performance.

MPC for humanoid robots often involves complex dynamic models that capture the robot's multi-body dynamics, contact forces, and environmental interactions. These models can be linearized around operating points or handled using nonlinear optimization techniques, depending on the required accuracy and computational constraints.

## Hands-on Activity

### Activity: Implementing a Simple Model Predictive Controller

In this activity, you'll implement a basic MPC controller for a simplified humanoid system, such as controlling the position of a single joint or maintaining balance in a simplified model.

<Tabs>
<TabItem value="python" label="Python Implementation">

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy.optimize import minimize
from typing import Tuple, List

class SimpleMPC:
    """Simple Model Predictive Controller for a humanoid joint"""

    def __init__(self, A: np.ndarray, B: np.ndarray, Q: np.ndarray, R: np.ndarray,
                 prediction_horizon: int = 10, control_horizon: int = 5):
        """
        Initialize MPC controller
        :param A: State transition matrix
        :param B: Control input matrix
        :param Q: State cost matrix
        :param R: Control cost matrix
        :param prediction_horizon: Number of steps to predict ahead
        :param control_horizon: Number of steps to optimize control
        """
        self.A = A  # State transition matrix
        self.B = B  # Control input matrix
        self.Q = Q  # State cost matrix
        self.R = R  # Control cost matrix
        self.prediction_horizon = prediction_horizon
        self.control_horizon = control_horizon

        # System dimensions
        self.n_states = A.shape[0]
        self.n_controls = B.shape[1]

    def predict_trajectory(self, x0: np.ndarray, U: np.ndarray) -> np.ndarray:
        """
        Predict state trajectory given initial state and control sequence
        :param x0: Initial state
        :param U: Control sequence (control_horizon x n_controls)
        :return: Predicted trajectory (prediction_horizon x n_states)
        """
        trajectory = np.zeros((self.prediction_horizon, self.n_states))
        x = x0.copy()

        for k in range(self.prediction_horizon):
            trajectory[k] = x

            # Apply control if within control horizon, else zero
            if k < self.control_horizon:
                u = U[k]
            else:
                u = np.zeros(self.n_controls)

            # Update state: x_{k+1} = A*x_k + B*u_k
            x = self.A @ x + self.B @ u

        return trajectory

    def cost_function(self, U_flat: np.ndarray, x0: np.ndarray, x_ref: np.ndarray) -> float:
        """
        Cost function to minimize
        :param U_flat: Flattened control sequence
        :param x0: Initial state
        :param x_ref: Reference trajectory
        :return: Total cost
        """
        # Reshape flattened control sequence
        U = U_flat.reshape((self.control_horizon, self.n_controls))

        # Predict trajectory
        trajectory = self.predict_trajectory(x0, U)

        # Calculate state cost
        state_cost = 0
        for k in range(self.prediction_horizon):
            error = trajectory[k] - x_ref[k] if k < len(x_ref) else trajectory[k]
            state_cost += error.T @ self.Q @ error

        # Calculate control cost
        control_cost = 0
        for k in range(self.control_horizon):
            control_cost += U[k].T @ self.R @ U[k]

        return state_cost + control_cost

    def control(self, x0: np.ndarray, x_ref: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
        """
        Compute optimal control action
        :param x0: Current state
        :param x_ref: Reference trajectory
        :return: Optimal control action and predicted trajectory
        """
        # Initialize control sequence
        U_init = np.zeros(self.control_horizon * self.n_controls)

        # Ensure reference trajectory is the right length
        if len(x_ref) < self.prediction_horizon:
            # Extend with the last reference point
            extended_ref = np.vstack([x_ref, np.tile(x_ref[-1], (self.prediction_horizon - len(x_ref), 1))])
        else:
            extended_ref = x_ref[:self.prediction_horizon]

        # Optimize control sequence
        result = minimize(
            self.cost_function,
            U_init,
            args=(x0, extended_ref),
            method='SLSQP',
            options={'disp': False}
        )

        # Reshape optimal control sequence
        U_opt = result.x.reshape((self.control_horizon, self.n_controls))

        # Return first control action and predicted trajectory
        predicted_trajectory = self.predict_trajectory(x0, U_opt)

        return U_opt[0], predicted_trajectory

class HumanoidBalanceMPC:
    """MPC controller for humanoid balance using inverted pendulum model"""

    def __init__(self, dt: float = 0.01, prediction_horizon: int = 20):
        """
        Initialize humanoid balance MPC
        :param dt: Time step
        :param prediction_horizon: Number of steps to predict ahead
        """
        self.dt = dt
        self.prediction_horizon = prediction_horizon

        # Simplified inverted pendulum model for balance
        # State: [x_pos, x_vel, y_pos, y_vel] - COM position and velocity
        A = np.array([
            [1, dt, 0, 0],
            [0, 1, 0, 0],
            [0, 0, 1, dt],
            [0, 0, 0, 1]
        ])

        # Control input: [F_x, F_y] - forces in x and y directions
        B = np.array([
            [0.5 * dt**2, 0],
            [dt, 0],
            [0, 0.5 * dt**2],
            [0, dt]
        ])

        # State cost: penalize deviation from balance point
        Q = np.diag([100, 10, 100, 10])  # Higher cost for position errors

        # Control cost: penalize large control efforts
        R = np.diag([1, 1])

        self.mpc = SimpleMPC(A, B, Q, R, prediction_horizon, prediction_horizon//2)

    def compute_control(self, com_pos: np.ndarray, com_vel: np.ndarray,
                       target_pos: np.ndarray = np.array([0, 0])) -> np.ndarray:
        """
        Compute control forces to maintain balance
        :param com_pos: Center of mass position [x, y]
        :param com_vel: Center of mass velocity [vx, vy]
        :param target_pos: Target balance position [x, y]
        :return: Control forces [Fx, Fy]
        """
        # Current state: [x_pos, x_vel, y_pos, y_vel]
        x0 = np.array([com_pos[0], com_vel[0], com_pos[1], com_vel[1]])

        # Reference trajectory: maintain target position
        x_ref = np.tile(np.array([target_pos[0], 0, target_pos[1], 0]),
                       (self.prediction_horizon, 1))

        # Compute control action
        control_action, predicted_trajectory = self.mpc.control(x0, x_ref)

        return control_action, predicted_trajectory

def simulate_balance_control():
    """Simulate humanoid balance control using MPC"""
    # Create MPC controller
    balance_controller = HumanoidBalanceMPC(dt=0.02, prediction_horizon=15)

    # Simulation parameters
    dt = 0.02
    simulation_time = 10.0
    steps = int(simulation_time / dt)

    # Initialize state
    com_pos = np.array([0.1, 0.05])  # Start slightly off balance
    com_vel = np.array([0.0, 0.0])

    # Store history for visualization
    pos_history = []
    vel_history = []
    force_history = []

    print("Simulating humanoid balance control with MPC...")
    print("Time\tCOM_X\tCOM_Y\tForce_X\tForce_Y")
    print("-" * 40)

    for i in range(steps):
        time = i * dt

        # Compute control forces
        forces, predicted_trajectory = balance_controller.compute_control(com_pos, com_vel)

        # Apply forces to the system (simplified dynamics)
        mass = 50.0  # Humanoid mass in kg
        com_acc = forces / mass

        # Update state
        com_vel += com_acc * dt
        com_pos += com_vel * dt

        # Store history
        pos_history.append(com_pos.copy())
        vel_history.append(com_vel.copy())
        force_history.append(forces.copy())

        # Print status every 50 steps
        if i % 50 == 0:
            print(f"{time:.2f}\t{com_pos[0]:.3f}\t{com_pos[1]:.3f}\t{forces[0]:.3f}\t{forces[1]:.3f}")

    # Convert to numpy arrays for plotting
    pos_history = np.array(pos_history)
    vel_history = np.array(vel_history)
    force_history = np.array(force_history)

    # Plot results
    fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(12, 10))

    # Plot COM position
    ax1.plot(pos_history[:, 0], label='COM X Position', linewidth=2)
    ax1.plot(pos_history[:, 1], label='COM Y Position', linewidth=2)
    ax1.axhline(y=0, color='k', linestyle='--', alpha=0.5, label='Balance Point')
    ax1.set_ylabel('Position (m)')
    ax1.set_title('Center of Mass Position Over Time')
    ax1.legend()
    ax1.grid(True, alpha=0.3)

    # Plot control forces
    ax2.plot(force_history[:, 0], label='Force X', linewidth=2)
    ax2.plot(force_history[:, 1], label='Force Y', linewidth=2)
    ax2.set_ylabel('Force (N)')
    ax2.set_title('Control Forces Applied')
    ax2.legend()
    ax2.grid(True, alpha=0.3)

    # Plot COM velocity
    ax3.plot(vel_history[:, 0], label='Velocity X', linewidth=2)
    ax3.plot(vel_history[:, 1], label='Velocity Y', linewidth=2)
    ax3.axhline(y=0, color='k', linestyle='--', alpha=0.5, label='Zero Velocity')
    ax3.set_ylabel('Velocity (m/s)')
    ax3.set_xlabel('Time Step')
    ax3.set_title('Center of Mass Velocity')
    ax3.legend()
    ax3.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.show()

    # Print final statistics
    final_pos_error = np.linalg.norm(com_pos)
    avg_control_effort = np.mean(np.linalg.norm(force_history, axis=1))

    print(f"\nFinal COM position error: {final_pos_error:.4f}m")
    print(f"Average control effort: {avg_control_effort:.4f}N")
    print("MPC balance control simulation completed!")

    return pos_history, force_history

# Run the simulation
pos_history, force_history = simulate_balance_control()
```

</TabItem>
</Tabs>

## Tools & Components Required

- Python 3.7 or higher
- NumPy for numerical computations
- SciPy for optimization algorithms
- Matplotlib for visualization
- Basic understanding of control theory and linear algebra

## Step-by-Step Instructions

1. **Initialize the MPC Controller**: Create a SimpleMPC class with system matrices (A, B) and cost matrices (Q, R)
2. **Implement Prediction**: Create a method to predict the system's trajectory given control inputs
3. **Define Cost Function**: Implement a cost function that penalizes state errors and control effort
4. **Optimize Control Sequence**: Use scipy.optimize.minimize to find the optimal control sequence
5. **Implement Rolling Horizon**: Apply only the first control action and repeat at each time step
6. **Test with Humanoid Balance**: Apply the controller to a simplified balance control problem
7. **Visualize Results**: Plot the system's response and control actions

## Code Snippets

### Core MPC Prediction Method

```python
def predict_trajectory(self, x0: np.ndarray, U: np.ndarray) -> np.ndarray:
    """
    Predict state trajectory given initial state and control sequence
    :param x0: Initial state
    :param U: Control sequence (control_horizon x n_controls)
    :return: Predicted trajectory (prediction_horizon x n_states)
    """
    trajectory = np.zeros((self.prediction_horizon, self.n_states))
    x = x0.copy()

    for k in range(self.prediction_horizon):
        trajectory[k] = x

        # Apply control if within control horizon, else zero
        if k < self.control_horizon:
            u = U[k]
        else:
            u = np.zeros(self.n_controls)

        # Update state: x_{k+1} = A*x_k + B*u_k
        x = self.A @ x + self.B @ u

    return trajectory
```

### Cost Function for Optimization

```python
def cost_function(self, U_flat: np.ndarray, x0: np.ndarray, x_ref: np.ndarray) -> float:
    """
    Cost function to minimize
    :param U_flat: Flattened control sequence
    :param x0: Initial state
    :param x_ref: Reference trajectory
    :return: Total cost
    """
    # Reshape flattened control sequence
    U = U_flat.reshape((self.control_horizon, self.n_controls))

    # Predict trajectory
    trajectory = self.predict_trajectory(x0, U)

    # Calculate state cost
    state_cost = 0
    for k in range(self.prediction_horizon):
        error = trajectory[k] - x_ref[k] if k < len(x_ref) else trajectory[k]
        state_cost += error.T @ self.Q @ error

    # Calculate control cost
    control_cost = 0
    for k in range(self.control_horizon):
        control_cost += U[k].T @ self.R @ U[k]

    return state_cost + control_cost
```

## Review Questions

1. What are the main advantages of Model Predictive Control over traditional PID controllers?
2. How does the prediction horizon affect MPC performance in humanoid robotics?
3. What computational challenges arise when implementing MPC on humanoid robots?
4. How can MPC handle multiple, potentially conflicting control objectives?
5. What role do constraints play in MPC for humanoid control systems?

## Mini Assessment

<Tabs>
<TabItem value="question1" label="Question 1">

**What is the primary advantage of MPC over traditional control methods for humanoid robots?**

A) Lower computational complexity
B) Ability to handle constraints and multiple objectives
C) Simpler implementation
D) Faster response times

<details>
<summary>Answer</summary>
B) Ability to handle constraints and multiple objectives - MPC can explicitly incorporate constraints and optimize multiple objectives simultaneously, which is crucial for humanoid robots.
</details>

</TabItem>

<TabItem value="question2" label="Question 2">

**What happens in the rolling horizon approach of MPC?**

A) The same control is applied repeatedly
B) Only the first control action is applied, then the process repeats
C) Control is computed only once for the entire trajectory
D) The horizon length decreases over time

<details>
<summary>Answer</summary>
B) Only the first control action is applied, then the process repeats - In rolling horizon, MPC solves the optimization at each time step and only applies the first action.
</details>

</TabItem>
</Tabs>

## Practical Task

Extend the MPC controller to handle a more complex humanoid model with multiple joints. Implement a controller that can maintain balance while tracking a desired walking trajectory. Consider how you would modify the state vector to include joint angles and velocities, and how the cost function would need to account for both balance and trajectory tracking objectives.

## Expected Outcomes

After completing this lesson, you should be able to:
- Implement basic MPC algorithms for humanoid control applications
- Understand the trade-offs between prediction horizon length and computational complexity
- Apply MPC to balance control and other humanoid robotics challenges
- Design cost functions that reflect multiple control objectives
- Evaluate the performance of MPC controllers in simulation